{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e64bae6",
   "metadata": {},
   "source": [
    "The goal was to build a Knowledge graph based retrieval augmented generation system.\n",
    "The knowledge graph was built from 'triplets' extracted from sentences.\n",
    "The triplets were of the form 'head', 'tail', 'relationship'.\n",
    "An example of how such a triplet can be extracted from a sentence is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caf10d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output:  Insulin  glucose metabolism  subject has role\n",
      "Extracted Triplets: [('Insulin', 'glucose metabolism', 'subject has role')]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"Babelscape/rebel-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Prompt-style input\n",
    "sentence = \"Insulin regulates glucose metabolism in the human body.\"\n",
    "prompt = f\"extract relation triplets from: {sentence}\"\n",
    "\n",
    "# Encode and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Raw Output:\", output_text)\n",
    "\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"  \")  # split on exactly two spaces\n",
    "        if len(parts) == 3:\n",
    "            head, tail, relation = map(str.strip, parts)\n",
    "            triplets.append((head, tail, relation))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "triplets = extract_triplets(output_text)\n",
    "print(\"Extracted Triplets:\", triplets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ba71e",
   "metadata": {},
   "source": [
    "The sentences or text was obtained by scraping the website https://patents.justia.com/. \n",
    "The code used for that is as given below.\n",
    "This data was saved to justia_patents.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ad1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Base URL (modify query if needed)\n",
    "BASE_URL = \"https://patents.justia.com/search?q=HVAC&page={}\"\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Open a CSV file to save results\n",
    "with open(\"justia_patents.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Title\", \"Link\", \"Abstract\"])  # Write the header\n",
    "\n",
    "    # Loop through the first 35 pages\n",
    "    for page in range(1, 36):\n",
    "        url = BASE_URL.format(page)\n",
    "        print(f\"üîç Scraping Page {page}...\")\n",
    "\n",
    "        # Send request to Justia Patents\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "        # Check if request was successful\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error: Page {page} could not be fetched (Status Code: {response.status_code})\")\n",
    "            continue  # Skip this page and move to the next\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the main container with patents\n",
    "        patent_container = soup.find(\"div\", id=\"search-results\", class_=\"wrapper\")\n",
    "\n",
    "        # Check if container exists\n",
    "        if not patent_container:\n",
    "            print(f\"‚ö†Ô∏è No patents found on page {page}.\")\n",
    "            continue\n",
    "\n",
    "        # Find all individual patent entries\n",
    "        patents = patent_container.find_all(\"li\")\n",
    "\n",
    "        # Loop through patents and extract information\n",
    "        for patent in patents:\n",
    "            title_elem = patent.find(\"h6\")  # Patent title\n",
    "            link_elem = patent.find(\"a\")  # Patent link\n",
    "            abstract_elem = patent.find(\"div\", class_=\"abstract\")  # Abstract\n",
    "\n",
    "            # Extract text safely\n",
    "            title = title_elem.text.strip() if title_elem else \"No title\"\n",
    "            link = \"https://patents.justia.com\" + link_elem[\"href\"] if link_elem else \"No link\"\n",
    "            abstract = abstract_elem.text.strip() if abstract_elem else \"No abstract available\"\n",
    "\n",
    "            # Save to CSV\n",
    "            writer.writerow([title, link, abstract])\n",
    "\n",
    "        print(f\"‚úÖ Page {page} scraped successfully!\")\n",
    "\n",
    "        # Add a short delay to avoid getting blocked\n",
    "        time.sleep(2)\n",
    "\n",
    "print(\"üéâ All 35 pages scraped successfully! Data saved to justia_patents.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73a4bb",
   "metadata": {},
   "source": [
    "Now, this data was cleaned, each sentence was segmented and triplets were extracted using the code given below. The model used for classifying the parts of the sentence was Babelscape/rebel-large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c88ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class RebelComponent:\n",
    "    def __init__(self, nlp, model_name='Babelscape/rebel-large', device=0):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        text = doc.text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# Don't add .to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(**inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        doc._.relations = self.extract_triplets(output_text)\n",
    "        return doc\n",
    "\n",
    "    def extract_triplets(self, text):\n",
    "        triplets = []\n",
    "        parts = text.split(\"<triplet>\")\n",
    "        for part in parts[1:]:\n",
    "            try:\n",
    "                head = part.split(\"<subj>\")[1].split(\"<obj>\")[0].strip()\n",
    "                relation = part.split(\"<obj>\")[1].split(\"<tail>\")[0].strip()\n",
    "                tail = part.split(\"<tail>\")[1].strip()\n",
    "                triplets.append({'head': head, 'relation': relation, 'tail': tail})\n",
    "            except IndexError:\n",
    "                continue\n",
    "        return triplets\n",
    "\n",
    "# Register the custom component in spaCy's pipeline\n",
    "@spacy.registry.misc(\"rebel_component\")\n",
    "def create_rebel_component():\n",
    "    return RebelComponent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea578608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Triplets extracted and saved to justia_triplets_all.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import spacy\n",
    "import csv\n",
    "\n",
    "# Load spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Babelscape REBEL model\n",
    "model_name = \"Babelscape/rebel-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Triplet parser based on double-space format\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"  \")\n",
    "        if len(parts) == 3:\n",
    "            head, tail, relation = map(str.strip, parts)\n",
    "            triplets.append((head, tail, relation))\n",
    "    return triplets\n",
    "\n",
    "# Read input CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\hvac_kg_project\\justia_patents.csv\")\n",
    "# Python uses 0-based indexing\n",
    "\n",
    "# Store output triplets here\n",
    "all_triplets = []\n",
    "\n",
    "# Process each row\n",
    "for idx, row in df.iterrows():\n",
    "    title = row.get(\"Title\", \"\")\n",
    "    link = row.get(\"Link\", \"\")\n",
    "    abstract_raw = row.get(\"Abstract\", \"\")\n",
    "\n",
    "    # Clean abstract text\n",
    "    if \"Abstract:\" in abstract_raw:\n",
    "        abstract_text = abstract_raw.split(\"Abstract:\")[-1].strip()\n",
    "    else:\n",
    "        abstract_text = abstract_raw.strip()\n",
    "\n",
    "    # Sentence segmentation using spaCy\n",
    "    doc = nlp(abstract_text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, max_length=256)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        triplets = extract_triplets(decoded_output)\n",
    "\n",
    "        for head, tail, relation in triplets:\n",
    "            all_triplets.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Sentence\": sentence,\n",
    "                \"Head\": head,\n",
    "                \"Tail\": tail,\n",
    "                \"Relation\": relation\n",
    "            })\n",
    "\n",
    "# Write to output CSV\n",
    "output_file = \"justia_triplets_all.csv\"\n",
    "with open(output_file, mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Title\", \"Link\", \"Sentence\", \"Head\", \"Tail\", \"Relation\"])\n",
    "    writer.writeheader()\n",
    "    for row in all_triplets:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"‚úÖ Triplets extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0b212",
   "metadata": {},
   "source": [
    "After saving these triplets, basic preprocessing was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aac6c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 1378\n",
      "After cleaning: 619\n",
      "‚úÖ Cleaned triplets saved to 'cleaned_triplets.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your triplets file\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\hvac_kg_project\\scripts\\justia_triplets_all.csv\")\n",
    "\n",
    "# Step 1: Drop rows with any missing values in head, tail, or relation\n",
    "df = df.dropna(subset=[\"Head\", \"Tail\", \"Relation\"])\n",
    "\n",
    "# Step 2: Normalize text ‚Äî strip and lowercase\n",
    "for col in [\"Head\", \"Tail\", \"Relation\"]:\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Step 3: Remove self-loops (where head == tail)\n",
    "df = df[df[\"Head\"] != df[\"Tail\"]]\n",
    "\n",
    "# Step 4: Remove very short or meaningless entries (e.g., 1-character)\n",
    "df = df[df[\"Head\"].str.len() > 1]\n",
    "df = df[df[\"Tail\"].str.len() > 1]\n",
    "df = df[df[\"Relation\"].str.len() > 1]\n",
    "\n",
    "# Step 5: Drop exact duplicates (same head, tail, relation)\n",
    "df_cleaned = df.drop_duplicates(subset=[\"Head\", \"Tail\", \"Relation\"])\n",
    "\n",
    "# Optional: reset index\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save the cleaned triplets to a new CSV\n",
    "df_cleaned.to_csv(r\"C:\\Users\\Admin\\Desktop\\hvac_kg_project\\scripts\\cleaned_triplets.csv\", index=False)\n",
    "print(\"Original rows:\", len(df))\n",
    "print(\"After cleaning:\", len(df_cleaned))\n",
    "\n",
    "print(\"‚úÖ Cleaned triplets saved to 'cleaned_triplets.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818f2fa",
   "metadata": {},
   "source": [
    "These cleaned triplets were used to create a knowledge graph in Neo4j, where 'Head' and 'Tail' where treated as entities and 'Relation' was treated as relations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092ca76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c807f31e",
   "metadata": {},
   "source": [
    "Langchain was installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b35e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.24-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain-community) (3.11.16)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 35.9 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.3.24-py3-none-any.whl (352 kB)\n",
      "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 23.4 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, mypy-extensions, marshmallow, httpx-sse, greenlet, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic-settings, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.40 dataclasses-json-0.6.7 greenlet-3.1.1 httpx-sse-0.4.0 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langsmith-0.3.24 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70466c31",
   "metadata": {},
   "source": [
    "After this, the connection with neo4j was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de7148f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "# If Neo4j Desktop is running and your DB is started, this should work\n",
    "graph = Graph(\"bolt://localhost:7687\")\n",
    "\n",
    "def test_graph_connection():\n",
    "    try:\n",
    "        result = graph.run(\"MATCH (n) RETURN n LIMIT 1\").data()\n",
    "        return \"Success!\" if result else \"Connected, but no data found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Neo4j connection error: {e}\"\n",
    "\n",
    "print(test_graph_connection())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27087e94",
   "metadata": {},
   "source": [
    "The llm's working was tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea88f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A patent is an intellectual property right granted by the government that gives its owner the exclusive rights to exclude others from making, using, selling, and importing an invention for a specified period of time. The purpose of a patent is to promote innovation and technological advancement by providing inventors with financial incentives for their inventions. To be eligible for a patent, the invention must meet certain requirements, such as being new, useful, and non-obvious. Patents can be granted for various types of inventions, including mechanical devices, chemical compounds, electrical circuits, software algorithms, and more.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "response = llm.invoke(\"What is a patent?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113c0d6",
   "metadata": {},
   "source": [
    "A function was defined to search the graph and retrieve context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54cc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_graph(graph, user_query):\n",
    "    # You can modify this to search Tail or Relation too\n",
    "    cypher_query = f\"\"\"\n",
    "    MATCH (h)-[r]->(t)\n",
    "    WHERE toLower(h.Name) CONTAINS toLower('{user_query}')\n",
    "       OR toLower(t.Name) CONTAINS toLower('{user_query}')\n",
    "       OR toLower(r.`relation`) CONTAINS toLower('{user_query}')\n",
    "    RETURN h.Name AS Head, r.relation AS Relation, t.Name AS Tail\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    results = graph.run(cypher_query).data()\n",
    "    \n",
    "    # Format result into a context string\n",
    "    context = \"\\n\".join([f\"{row['Head']} --{row['Relation']}--> {row['Tail']}\" for row in results])\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696fba0",
   "metadata": {},
   "source": [
    "Whether the function is working properly was tested with the example given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01c1f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Retrieved context:\n",
      " hvac energy analytics engine --facet of--> hvac system\n",
      "hvac energy analytics engine --facet of--> hvac system\n",
      "expressive decision tables --instance of--> engine\n",
      "expressive decision tables --instance of--> engine\n",
      "start-stop --subclass of--> engine operation\n",
      "start-stop --subclass of--> engine operation\n",
      "powertrain control module --use--> engine operation\n",
      "powertrain control module --use--> engine operation\n",
      "parametric model --instance of--> knowledge-based engineering library\n",
      "parametric model --instance of--> knowledge-based engineering library\n"
     ]
    }
   ],
   "source": [
    "user_query = \"engine\"  # You can replace this with any user input\n",
    "context = search_graph(graph, user_query)\n",
    "print(\"üìò Retrieved context:\\n\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69e35d",
   "metadata": {},
   "source": [
    "Then, it was tested whether the llm is answering the queries correctly and with reference to the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f3fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer from LLM:\n",
      "  In the given context, an \"engine\" is a facet of both HVAC (Heating, Ventilation, and Air Conditioning) systems and is also associated with powertrain control modules. The \"engine\" concept is further subclassified into \"engine operation\", which includes \"start-stop\". Additionally, it interacts with \"parametric models\", which are instances of the \"knowledge-based engineering library\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Step 1: Connect to Ollama (make sure it's running)\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Step 2: Create prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Use the context below to answer the question.\n",
    "If the context does not contain enough information, say \"I don't know based on the current knowledge.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 3: Wrap into a chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Step 4: Use your previous context + user query\n",
    "response = qa_chain.run({\n",
    "    \"context\": context,  # this comes from your graph search\n",
    "    \"question\": user_query\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Answer from LLM:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7895b1",
   "metadata": {},
   "source": [
    "Whether or not gradio is working was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc9bfcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from py2neo import Graph\n",
    "\n",
    "def test_connection(question):\n",
    "    try:\n",
    "        graph = Graph(\"bolt://localhost:7687\")  # No auth for desktop Neo4j\n",
    "        result = graph.run(\"MATCH (n) RETURN COUNT(n) AS count\").data()\n",
    "        count = result[0]['count']\n",
    "        return f\"Neo4j is connected ‚úÖ. Your graph has {count} nodes.\\nYou asked: {question}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error connecting to Neo4j: {str(e)}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=test_connection,\n",
    "    inputs=gr.Textbox(label=\"Test Question\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Neo4j Test\",\n",
    "    description=\"Checks if Neo4j Desktop connection works\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e13e35",
   "metadata": {},
   "source": [
    "All of the above functionalities were integrated to get one user interface where you can enter topic specific queries and the llm with reference to the knowledge graph, gives you answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e510258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from py2neo import Graph\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ---- SETUP ----\n",
    "\n",
    "# Connect to Neo4j Desktop (adjust the connection if needed)\n",
    "graph = Graph(\"bolt://localhost:7687\")  # No auth assumed for Neo4j Desktop\n",
    "\n",
    "# Connect to local Ollama model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Prompt template for LLM\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"user_query\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Use the context below to answer the question.\n",
    "If the context does not contain enough information, say \"I don't know based on the current knowledge.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# ---- GRAPH SEARCH ----\n",
    "def search_graph(graph, user_query):\n",
    "    # You can modify this to search Tail or Relation too\n",
    "    cypher_query = f\"\"\"\n",
    "    MATCH (h)-[r]->(t)\n",
    "    WHERE toLower(h.Name) CONTAINS toLower('{user_query}')\n",
    "       OR toLower(t.Name) CONTAINS toLower('{user_query}')\n",
    "       OR toLower(r.`relation`) CONTAINS toLower('{user_query}')\n",
    "    RETURN h.Name AS Head, r.relation AS Relation, t.Name AS Tail\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    results = graph.run(cypher_query).data()\n",
    "    \n",
    "    # Format result into a context string\n",
    "    context = \"\\n\".join([f\"{row['Head']} --{row['Relation']}--> {row['Tail']}\" for row in results])\n",
    "    return context\n",
    "\n",
    "def answer_question(user_query):\n",
    "    context = search_graph(graph, user_query)\n",
    "    if context.startswith(\"Error\") or \"No matching\" in context:\n",
    "        return f\"üîç \" + context\n",
    "    response = qa_chain.run({\n",
    "        \"context\": context,\n",
    "        \"user_query\": user_query\n",
    "    })\n",
    "    return f\"ü§ñ Answer: {response}\\n\\nüìö Context Used:\\n{context}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(label=\"Ask a question about your knowledge graph\"),\n",
    "    outputs=gr.Textbox(label=\"Answer with context\"),\n",
    "    title=\"Neo4j + Ollama RAG Chatbot\",\n",
    "    description=\"Ask any question. The bot searches your Neo4j knowledge graph for relevant facts, then generates an answer using a local CPU-friendly LLM.\",\n",
    ")\n",
    "\n",
    "# Launch\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
